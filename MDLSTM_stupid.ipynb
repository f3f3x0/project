{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the EMNIST dataset\n",
    "\n",
    "train_data = torchvision.datasets.EMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True,\n",
    "    split='letters'\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.EMNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=False,\n",
    "    split='letters'     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Label: t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157,\n",
       "         0.0157, 0.0353, 0.1255, 0.1255, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0392, 0.4431,\n",
       "         0.4980, 0.5490, 0.7961, 0.7961, 0.3059, 0.0275, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0078, 0.0353, 0.1255, 0.2000, 0.5020, 0.9529,\n",
       "         0.9804, 0.9804, 0.9961, 0.9961, 0.7882, 0.1255, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0157,\n",
       "         0.0157, 0.0157, 0.0863, 0.3216, 0.5451, 0.8000, 0.8706, 0.9608, 0.9804,\n",
       "         0.9647, 0.9373, 0.9922, 1.0000, 0.8510, 0.1451, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0157, 0.0353, 0.1255, 0.1529, 0.3216, 0.4902,\n",
       "         0.4980, 0.4980, 0.6784, 0.9098, 0.9804, 0.9922, 0.9922, 0.9569, 0.5529,\n",
       "         0.4549, 0.4471, 0.9176, 0.9961, 0.8000, 0.1255, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0078, 0.2980, 0.7882, 0.8706, 0.9608, 0.9804, 0.9882, 0.9961,\n",
       "         0.9961, 0.9922, 0.9804, 0.9608, 0.8157, 0.5490, 0.4471, 0.1804, 0.0039,\n",
       "         0.0078, 0.3216, 0.9137, 0.9922, 0.4510, 0.0157, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0118, 0.4353, 0.9412, 0.9804, 0.9804, 0.9137, 0.8510, 0.8510,\n",
       "         0.8510, 0.8000, 0.5059, 0.4471, 0.1804, 0.0353, 0.0157, 0.0000, 0.0000,\n",
       "         0.0353, 0.5451, 0.9804, 0.9647, 0.1804, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.1255, 0.4431, 0.4980, 0.4980, 0.3216, 0.1529, 0.1451,\n",
       "         0.1451, 0.1255, 0.0196, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.1804, 0.8157, 0.9922, 0.8627, 0.0353, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0157, 0.0157, 0.0157, 0.0078, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0314,\n",
       "         0.4980, 0.9647, 0.9843, 0.6667, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804,\n",
       "         0.8157, 0.9922, 0.8627, 0.3098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.5020,\n",
       "         0.9608, 0.9569, 0.4980, 0.0392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1294, 0.8667,\n",
       "         0.9843, 0.6863, 0.1333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0078, 0.0196, 0.0863, 0.3294, 0.7333, 0.9961,\n",
       "         0.9451, 0.6235, 0.5020, 0.4471, 0.1490, 0.0784, 0.0039, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0784, 0.1451,\n",
       "         0.1451, 0.1451, 0.1529, 0.3216, 0.4980, 0.6784, 0.9098, 0.9882, 0.9961,\n",
       "         0.9961, 0.9843, 0.9804, 0.9608, 0.8471, 0.6235, 0.1843, 0.0196, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1294, 0.6196, 0.8431,\n",
       "         0.8510, 0.8510, 0.8510, 0.9137, 0.9765, 0.9882, 0.9961, 0.9961, 0.9804,\n",
       "         0.9804, 0.9804, 0.9804, 0.9804, 0.9922, 0.9686, 0.7451, 0.1294, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4431, 0.9529, 0.9961,\n",
       "         0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9882, 0.9765, 0.9098, 0.5490,\n",
       "         0.4980, 0.4980, 0.4980, 0.5059, 0.9176, 0.9961, 0.9137, 0.3216, 0.0078,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4235, 0.8353, 0.8510,\n",
       "         0.8510, 0.8510, 0.8510, 0.8510, 0.8431, 0.6745, 0.4980, 0.3216, 0.0353,\n",
       "         0.0157, 0.0157, 0.0157, 0.0275, 0.6784, 0.9882, 0.9765, 0.4902, 0.0157,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1451, 0.1451,\n",
       "         0.1451, 0.1451, 0.1451, 0.1451, 0.1451, 0.0824, 0.0196, 0.0078, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.5098, 0.9804, 0.9804, 0.4980, 0.0157,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0078, 0.6745, 0.9882, 0.9804, 0.4980, 0.0157,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0039, 0.1412, 0.9176, 0.9961, 0.9098, 0.3216, 0.0078,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0784, 0.6196, 0.9922, 0.9843, 0.6706, 0.0863, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1255, 0.7843, 0.9922, 0.9020, 0.3255, 0.0118, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0275, 0.3529, 0.7922, 0.4784, 0.0784, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0275, 0.1255, 0.0314, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train = train_data.data / 255\n",
    "y_train = train_data.targets - 1\n",
    "\n",
    "# Show some random image of a character and its label\n",
    "\n",
    "img_index = 16\n",
    "img = x_train[img_index]\n",
    "print(\"Image Label: \" + str(chr(y_train[img_index]+96)))\n",
    "plt.imshow(img.reshape((28,28)))\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  1.,   2.,   3.,   4.,  29.,  30.,  31.,  32.,  57.,  58.,  59.,\n",
       "            60.,  85.,  86.,  87.,  88.],\n",
       "          [  5.,   6.,   7.,   8.,  33.,  34.,  35.,  36.,  61.,  62.,  63.,\n",
       "            64.,  89.,  90.,  91.,  92.],\n",
       "          [  9.,  10.,  11.,  12.,  37.,  38.,  39.,  40.,  65.,  66.,  67.,\n",
       "            68.,  93.,  94.,  95.,  96.],\n",
       "          [ 13.,  14.,  15.,  16.,  41.,  42.,  43.,  44.,  69.,  70.,  71.,\n",
       "            72.,  97.,  98.,  99., 100.],\n",
       "          [ 17.,  18.,  19.,  20.,  45.,  46.,  47.,  48.,  73.,  74.,  75.,\n",
       "            76., 101., 102., 103., 104.],\n",
       "          [ 21.,  22.,  23.,  24.,  49.,  50.,  51.,  52.,  77.,  78.,  79.,\n",
       "            80., 105., 106., 107., 108.],\n",
       "          [ 25.,  26.,  27.,  28.,  53.,  54.,  55.,  56.,  81.,  82.,  83.,\n",
       "            84., 109., 110., 111., 112.]],\n",
       "\n",
       "         [[113., 114., 115., 116., 141., 142., 143., 144., 169., 170., 171.,\n",
       "           172., 197., 198., 199., 200.],\n",
       "          [117., 118., 119., 120., 145., 146., 147., 148., 173., 174., 175.,\n",
       "           176., 201., 202., 203., 204.],\n",
       "          [121., 122., 123., 124., 149., 150., 151., 152., 177., 178., 179.,\n",
       "           180., 205., 206., 207., 208.],\n",
       "          [125., 126., 127., 128., 153., 154., 155., 156., 181., 182., 183.,\n",
       "           184., 209., 210., 211., 212.],\n",
       "          [129., 130., 131., 132., 157., 158., 159., 160., 185., 186., 187.,\n",
       "           188., 213., 214., 215., 216.],\n",
       "          [133., 134., 135., 136., 161., 162., 163., 164., 189., 190., 191.,\n",
       "           192., 217., 218., 219., 220.],\n",
       "          [137., 138., 139., 140., 165., 166., 167., 168., 193., 194., 195.,\n",
       "           196., 221., 222., 223., 224.]],\n",
       "\n",
       "         [[225., 226., 227., 228., 253., 254., 255., 256., 281., 282., 283.,\n",
       "           284., 309., 310., 311., 312.],\n",
       "          [229., 230., 231., 232., 257., 258., 259., 260., 285., 286., 287.,\n",
       "           288., 313., 314., 315., 316.],\n",
       "          [233., 234., 235., 236., 261., 262., 263., 264., 289., 290., 291.,\n",
       "           292., 317., 318., 319., 320.],\n",
       "          [237., 238., 239., 240., 265., 266., 267., 268., 293., 294., 295.,\n",
       "           296., 321., 322., 323., 324.],\n",
       "          [241., 242., 243., 244., 269., 270., 271., 272., 297., 298., 299.,\n",
       "           300., 325., 326., 327., 328.],\n",
       "          [245., 246., 247., 248., 273., 274., 275., 276., 301., 302., 303.,\n",
       "           304., 329., 330., 331., 332.],\n",
       "          [249., 250., 251., 252., 277., 278., 279., 280., 305., 306., 307.,\n",
       "           308., 333., 334., 335., 336.]],\n",
       "\n",
       "         [[337., 338., 339., 340., 365., 366., 367., 368., 393., 394., 395.,\n",
       "           396., 421., 422., 423., 424.],\n",
       "          [341., 342., 343., 344., 369., 370., 371., 372., 397., 398., 399.,\n",
       "           400., 425., 426., 427., 428.],\n",
       "          [345., 346., 347., 348., 373., 374., 375., 376., 401., 402., 403.,\n",
       "           404., 429., 430., 431., 432.],\n",
       "          [349., 350., 351., 352., 377., 378., 379., 380., 405., 406., 407.,\n",
       "           408., 433., 434., 435., 436.],\n",
       "          [353., 354., 355., 356., 381., 382., 383., 384., 409., 410., 411.,\n",
       "           412., 437., 438., 439., 440.],\n",
       "          [357., 358., 359., 360., 385., 386., 387., 388., 413., 414., 415.,\n",
       "           416., 441., 442., 443., 444.],\n",
       "          [361., 362., 363., 364., 389., 390., 391., 392., 417., 418., 419.,\n",
       "           420., 445., 446., 447., 448.]],\n",
       "\n",
       "         [[449., 450., 451., 452., 477., 478., 479., 480., 505., 506., 507.,\n",
       "           508., 533., 534., 535., 536.],\n",
       "          [453., 454., 455., 456., 481., 482., 483., 484., 509., 510., 511.,\n",
       "           512., 537., 538., 539., 540.],\n",
       "          [457., 458., 459., 460., 485., 486., 487., 488., 513., 514., 515.,\n",
       "           516., 541., 542., 543., 544.],\n",
       "          [461., 462., 463., 464., 489., 490., 491., 492., 517., 518., 519.,\n",
       "           520., 545., 546., 547., 548.],\n",
       "          [465., 466., 467., 468., 493., 494., 495., 496., 521., 522., 523.,\n",
       "           524., 549., 550., 551., 552.],\n",
       "          [469., 470., 471., 472., 497., 498., 499., 500., 525., 526., 527.,\n",
       "           528., 553., 554., 555., 556.],\n",
       "          [473., 474., 475., 476., 501., 502., 503., 504., 529., 530., 531.,\n",
       "           532., 557., 558., 559., 560.]],\n",
       "\n",
       "         [[561., 562., 563., 564., 589., 590., 591., 592., 617., 618., 619.,\n",
       "           620., 645., 646., 647., 648.],\n",
       "          [565., 566., 567., 568., 593., 594., 595., 596., 621., 622., 623.,\n",
       "           624., 649., 650., 651., 652.],\n",
       "          [569., 570., 571., 572., 597., 598., 599., 600., 625., 626., 627.,\n",
       "           628., 653., 654., 655., 656.],\n",
       "          [573., 574., 575., 576., 601., 602., 603., 604., 629., 630., 631.,\n",
       "           632., 657., 658., 659., 660.],\n",
       "          [577., 578., 579., 580., 605., 606., 607., 608., 633., 634., 635.,\n",
       "           636., 661., 662., 663., 664.],\n",
       "          [581., 582., 583., 584., 609., 610., 611., 612., 637., 638., 639.,\n",
       "           640., 665., 666., 667., 668.],\n",
       "          [585., 586., 587., 588., 613., 614., 615., 616., 641., 642., 643.,\n",
       "           644., 669., 670., 671., 672.]],\n",
       "\n",
       "         [[673., 674., 675., 676., 701., 702., 703., 704., 729., 730., 731.,\n",
       "           732., 757., 758., 759., 760.],\n",
       "          [677., 678., 679., 680., 705., 706., 707., 708., 733., 734., 735.,\n",
       "           736., 761., 762., 763., 764.],\n",
       "          [681., 682., 683., 684., 709., 710., 711., 712., 737., 738., 739.,\n",
       "           740., 765., 766., 767., 768.],\n",
       "          [685., 686., 687., 688., 713., 714., 715., 716., 741., 742., 743.,\n",
       "           744., 769., 770., 771., 772.],\n",
       "          [689., 690., 691., 692., 717., 718., 719., 720., 745., 746., 747.,\n",
       "           748., 773., 774., 775., 776.],\n",
       "          [693., 694., 695., 696., 721., 722., 723., 724., 749., 750., 751.,\n",
       "           752., 777., 778., 779., 780.],\n",
       "          [697., 698., 699., 700., 725., 726., 727., 728., 753., 754., 755.,\n",
       "           756., 781., 782., 783., 784.]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[x + 1 for x in range(28 * y, 28 * (y + 1))] for y in range(28)]).view(1, 1, 28, 28).float()\n",
    "c = F.unfold(a, kernel_size=(4, 4), stride=4).transpose(1, 2).view(1, 7, 7, 16)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0157, 0.0157, 0.0353, 0.0392, 0.4431, 0.4980, 0.5490],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.1255, 0.1255, 0.0118, 0.0000, 0.7961, 0.7961, 0.3059, 0.0275],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0157, 0.0000, 0.0078, 0.2980, 0.7882],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078,\n",
       "          0.0353, 0.1255, 0.1529, 0.3216, 0.8706, 0.9608, 0.9804, 0.9882],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.0157, 0.0157, 0.0863,\n",
       "          0.4902, 0.4980, 0.4980, 0.6784, 0.9961, 0.9961, 0.9922, 0.9804],\n",
       "         [0.0078, 0.0353, 0.1255, 0.2000, 0.3216, 0.5451, 0.8000, 0.8706,\n",
       "          0.9098, 0.9804, 0.9922, 0.9922, 0.9608, 0.8157, 0.5490, 0.4471],\n",
       "         [0.5020, 0.9529, 0.9804, 0.9804, 0.9608, 0.9804, 0.9647, 0.9373,\n",
       "          0.9569, 0.5529, 0.4549, 0.4471, 0.1804, 0.0039, 0.0078, 0.3216],\n",
       "         [0.9961, 0.9961, 0.7882, 0.1255, 0.9922, 1.0000, 0.8510, 0.1451,\n",
       "          0.9176, 0.9961, 0.8000, 0.1255, 0.9137, 0.9922, 0.4510, 0.0157],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0118, 0.4353, 0.9412, 0.0000, 0.0000, 0.1255, 0.4431,\n",
       "          0.0000, 0.0000, 0.0000, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.9804, 0.9804, 0.9137, 0.8510, 0.4980, 0.4980, 0.3216, 0.1529,\n",
       "          0.0157, 0.0157, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8510, 0.8510, 0.8000, 0.5059, 0.1451, 0.1451, 0.1255, 0.0196,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4471, 0.1804, 0.0353, 0.0157, 0.0157, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0353, 0.5451, 0.0000, 0.0000, 0.1804, 0.8157,\n",
       "          0.0000, 0.0314, 0.4980, 0.9647, 0.0000, 0.1804, 0.8157, 0.9922],\n",
       "         [0.9804, 0.9647, 0.1804, 0.0000, 0.9922, 0.8627, 0.0353, 0.0000,\n",
       "          0.9843, 0.6667, 0.0078, 0.0000, 0.8627, 0.3098, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0784],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.1451, 0.1451, 0.1451, 0.1529],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0078, 0.0196, 0.0863, 0.3294, 0.3216, 0.4980, 0.6784, 0.9098],\n",
       "         [0.0039, 0.5020, 0.9608, 0.9569, 0.1294, 0.8667, 0.9843, 0.6863,\n",
       "          0.7333, 0.9961, 0.9451, 0.6235, 0.9882, 0.9961, 0.9961, 0.9843],\n",
       "         [0.4980, 0.0392, 0.0000, 0.0000, 0.1333, 0.0000, 0.0000, 0.0000,\n",
       "          0.5020, 0.4471, 0.1490, 0.0784, 0.9804, 0.9608, 0.8471, 0.6235],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0039, 0.0000, 0.0000, 0.0000, 0.1843, 0.0196, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.1294, 0.6196, 0.0000, 0.0000, 0.4431, 0.9529,\n",
       "          0.0000, 0.0000, 0.4235, 0.8353, 0.0000, 0.0000, 0.0706, 0.1451],\n",
       "         [0.8431, 0.8510, 0.8510, 0.8510, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "          0.8510, 0.8510, 0.8510, 0.8510, 0.1451, 0.1451, 0.1451, 0.1451],\n",
       "         [0.9137, 0.9765, 0.9882, 0.9961, 0.9961, 0.9961, 0.9882, 0.9765,\n",
       "          0.8510, 0.8431, 0.6745, 0.4980, 0.1451, 0.1451, 0.0824, 0.0196],\n",
       "         [0.9961, 0.9804, 0.9804, 0.9804, 0.9098, 0.5490, 0.4980, 0.4980,\n",
       "          0.3216, 0.0353, 0.0157, 0.0157, 0.0078, 0.0000, 0.0000, 0.0000],\n",
       "         [0.9804, 0.9804, 0.9922, 0.9686, 0.4980, 0.5059, 0.9176, 0.9961,\n",
       "          0.0157, 0.0275, 0.6784, 0.9882, 0.0000, 0.0000, 0.5098, 0.9804],\n",
       "         [0.7451, 0.1294, 0.0000, 0.0000, 0.9137, 0.3216, 0.0078, 0.0000,\n",
       "          0.9765, 0.4902, 0.0157, 0.0000, 0.9804, 0.4980, 0.0157, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0078, 0.6745, 0.9882, 0.0039, 0.1412, 0.9176, 0.9961,\n",
       "          0.0784, 0.6196, 0.9922, 0.9843, 0.1255, 0.7843, 0.9922, 0.9020],\n",
       "         [0.9804, 0.4980, 0.0157, 0.0000, 0.9098, 0.3216, 0.0078, 0.0000,\n",
       "          0.6706, 0.0863, 0.0000, 0.0000, 0.3255, 0.0118, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0275, 0.3529, 0.7922, 0.4784, 0.0000, 0.0275, 0.1255, 0.0314,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0784, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose a particular image and convert to the right type\n",
    "temp_image = F.unfold(img.view(1, 1, 28, 28), kernel_size=(4, 4), stride=4).transpose(1, 2).view(7, 7, 16)\n",
    "temp_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoDimensionalLSTM_fixed_direction:\n",
    "    \"\"\"A 2-D LSTM scanning in the given direction.\n",
    "       The input should be a 3-D tensor of shape row*col*input_feature.\n",
    "       The output should be a 2-D tensor of shape row*(col*hidden_units).\n",
    "       \"\"\"\n",
    "    def __init__(self, input_features, hidden_units, rows, cols, row_decre, col_decre):\n",
    "        self.input_features = input_features\n",
    "        self.hidden_units = hidden_units\n",
    "        self.rows, self.cols = rows, cols\n",
    "        self.row_size = len(rows)\n",
    "        self.col_size = len(cols)\n",
    "        self.row_decre = row_decre\n",
    "        self.col_decre = col_decre\n",
    "        self.row_init, self.col_init = rows[0], cols[0]\n",
    "\n",
    "        # To avoid scattering of tanh(), initialization issue needs to be solved!!!\n",
    "\n",
    "        # Input gate\n",
    "        self.weight_input_gate = torch.randn((input_features, hidden_units)) * 0.01\n",
    "        self.weight_input_state = torch.randn((hidden_units, hidden_units)) * 0.01\n",
    "        self.weight_input_cellout = torch.randn((2, hidden_units, hidden_units)) * 0.01\n",
    "        self.bias_input_gate = torch.randn(hidden_units) * 0.01\n",
    "\n",
    "        # Forget gate\n",
    "        self.weight_forget_gate = torch.randn((2, input_features, hidden_units)) * 0.01\n",
    "        self.weight_forget_cellout = torch.randn((2, 2, hidden_units, hidden_units)) * 0.01\n",
    "        self.weight_forget_state = torch.randn((2, hidden_units, hidden_units)) * 0.01\n",
    "        self.bias_forget_gate = torch.randn((2, hidden_units)) * 0.01\n",
    "\n",
    "        # Cell\n",
    "        self.weight_cell = torch.randn((input_features, hidden_units)) * 0.01\n",
    "        self.weight_cell_cellout = torch.randn((2, hidden_units, hidden_units)) * 0.01\n",
    "        self.bias_cell = torch.randn(hidden_units) * 0.01\n",
    "\n",
    "        # Output gate\n",
    "        self.weight_output_gate = torch.randn((input_features, hidden_units)) * 0.01\n",
    "        self.weight_output_cellout = torch.randn((2, hidden_units, hidden_units)) * 0.01\n",
    "        self.weight_output_state = torch.randn((hidden_units, hidden_units)) * 0.01\n",
    "        self.bias_output_gate = torch.randn(hidden_units) * 0.01\n",
    "\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight_input_gate, \n",
    "                self.weight_input_state, \n",
    "                self.weight_input_cellout,\n",
    "                self.bias_input_gate,  # Input gate\n",
    "                self.weight_forget_gate, \n",
    "                self.weight_forget_cellout,\n",
    "                self.weight_forget_state, \n",
    "                self.bias_forget_gate,  # Forget gate\n",
    "                self.weight_cell, \n",
    "                self.weight_cell_cellout,\n",
    "                self.bias_cell,  # Cell\n",
    "                self.weight_output_gate, \n",
    "                self.weight_output_cellout, \n",
    "                self.weight_output_state, \n",
    "                self.bias_output_gate] # Output gate\n",
    "    \n",
    "    def __call__(self, input_image):\n",
    "        states = np.zeros((len(self.rows), len(self.cols))).tolist()\n",
    "        cell_outputs = np.zeros((len(self.rows), len(self.cols))).tolist()\n",
    "\n",
    "        for row in self.rows:\n",
    "            for col in self.cols:\n",
    "                _input = input_image[row][col]\n",
    "                row_m = row + self.row_decre\n",
    "                col_m = col + self.col_decre\n",
    "                \n",
    "                # Deal with input gate\n",
    "                input_gate = _input @ self.weight_input_gate + self.bias_input_gate\n",
    "                if row != self.row_init:\n",
    "                    input_gate += states[row_m][col] @ self.weight_input_state \\\n",
    "                                + cell_outputs[row_m][col] @ self.weight_input_cellout[0]\n",
    "                if col != self.col_init:\n",
    "                    input_gate += states[row][col_m] @ self.weight_input_state \\\n",
    "                                + cell_outputs[row][col_m] @ self.weight_input_cellout[1]\n",
    "                input_gate = input_gate.sigmoid()\n",
    "\n",
    "                # Deal with forget gate\n",
    "                forget_gates = []\n",
    "                for dim in range(2):\n",
    "                    forget_gate = _input @ self.weight_forget_gate[dim] + self.bias_forget_gate[dim]\n",
    "                    if row != self.row_init:\n",
    "                        forget_gate += cell_outputs[row_m][col] @ self.weight_forget_cellout[0][dim]\n",
    "                    if col != self.col_init:\n",
    "                        forget_gate += cell_outputs[row][col_m] @ self.weight_forget_cellout[1][dim]\n",
    "                    if dim == 0 and row != self.row_init:\n",
    "                        forget_gate += states[row_m][col] @ self.weight_forget_state[dim]\n",
    "                    if dim == 1 and col != self.col_init:\n",
    "                        forget_gate += states[row][col_m] @ self.weight_forget_state[dim]\n",
    "                    forget_gates.append(forget_gate.sigmoid())\n",
    "\n",
    "                # Deal with cell\n",
    "                cell = _input @ self.weight_cell + self.bias_cell\n",
    "                if row != self.row_init:\n",
    "                    cell += cell_outputs[row_m][col] @ self.weight_cell_cellout[0]\n",
    "                if col != self.col_init:\n",
    "                    cell += cell_outputs[row][col_m] @ self.weight_cell_cellout[1]\n",
    "\n",
    "                # Deal with state\n",
    "                state = input_gate * cell.tanh()\n",
    "                if row != self.row_init:\n",
    "                    state += states[row_m][col] @ forget_gates[0]\n",
    "                if col != self.col_init:\n",
    "                    state += states[row][col_m] @ forget_gates[1]\n",
    "                states[row][col] = state\n",
    "\n",
    "                # Deal with output gate\n",
    "                output_gate = _input @ self.weight_output_gate + self.bias_output_gate + state @ self.weight_output_state\n",
    "                if row != self.row_init:\n",
    "                    output_gate += cell_outputs[row_m][col] @ self.weight_output_cellout[0]\n",
    "                if col != self.col_init:\n",
    "                    output_gate += cell_outputs[row][col_m] @ self.weight_output_cellout[1]\n",
    "                output_gate = output_gate.sigmoid()\n",
    "\n",
    "                # Deal with cell output\n",
    "                cell_outputs[row][col] = output_gate * state.tanh()\n",
    "\n",
    "        return torch.stack([torch.stack(row_cell_outputs) for row_cell_outputs in cell_outputs]).view(self.row_size * self.col_size, self.hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoDimensionalLSTM:\n",
    "    def __init__(self, input_features, hidden_units, row_size, col_size):\n",
    "        self.hidden_units = hidden_units\n",
    "        self.row_size, self.col_size = row_size, col_size\n",
    "        self.top_left = TwoDimensionalLSTM_fixed_direction(input_features,\n",
    "                                                           hidden_units,\n",
    "                                                           np.arange(row_size),\n",
    "                                                           np.arange(col_size),\n",
    "                                                           -1, -1)\n",
    "        self.top_right = TwoDimensionalLSTM_fixed_direction(input_features,\n",
    "                                                            hidden_units,\n",
    "                                                            np.arange(row_size),\n",
    "                                                            np.arange(col_size - 1, -1, -1),\n",
    "                                                            -1, 1)\n",
    "        self.down_left = TwoDimensionalLSTM_fixed_direction(input_features,\n",
    "                                                            hidden_units,\n",
    "                                                            np.arange(row_size - 1, -1, -1),\n",
    "                                                            np.arange(col_size),\n",
    "                                                            1, -1)\n",
    "        self.down_right = TwoDimensionalLSTM_fixed_direction(input_features,\n",
    "                                                             hidden_units,\n",
    "                                                             np.arange(row_size - 1, -1, -1),\n",
    "                                                             np.arange(col_size - 1, -1, -1),\n",
    "                                                             1, 1)\n",
    "        self.weight = torch.randn((4 * hidden_units, hidden_units)) * 0.01\n",
    "        self.bias = torch.randn(hidden_units) * 0.01\n",
    "        self.parameter = self.top_left.parameters() + self.top_right.parameters()\\\n",
    "                        + self.down_left.parameters() + self.down_right.parameters() + [self.weight, self.bias]\n",
    "        \n",
    "    def parameters(self):\n",
    "        return self.parameter\n",
    "    \n",
    "    def __call__(self, input_image):\n",
    "        temp = torch.cat((self.top_left(input_image), self.top_right(input_image),\n",
    "                          self.down_left(input_image), self.down_right(input_image)), dim=1)\n",
    "        temp = (temp @ self.weight + self.bias).tanh()\n",
    "        return F.fold(temp.transpose(0, 1).view(1, self.hidden_units, self.row_size * self.col_size),\n",
    "                      (self.row_size, self.col_size * self.hidden_units),\n",
    "                      (1, self.hidden_units),\n",
    "                      stride=(1, self.hidden_units))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters Settings 32 * 32\n",
    "\n",
    "parameters = []\n",
    "\n",
    "# The first layer of mdlstm, transfer to 8 * (8 * 2)\n",
    "first_layer = TwoDimensionalLSTM(16, 2, 8, 8)\n",
    "parameters.extend(first_layer.parameters())\n",
    "\n",
    "# The second layer of mdlstm, transfer to 4 * (8 * 4)\n",
    "second_layer = TwoDimensionalLSTM(4, 4, 4, 8)\n",
    "parameters.extend(second_layer.parameters())\n",
    "\n",
    "# The third layer of mklstm transfer to 2 * (16 * 8)\n",
    "third_layer = TwoDimensionalLSTM(4, 8, 2, 16)\n",
    "parameters.extend(third_layer.parameters())\n",
    "\n",
    "# The fourth layer of mdlstm, transfer to  1 * (64 * 5)\n",
    "fourth_layer = TwoDimensionalLSTM(4, 5, 1, 64)\n",
    "parameters.extend(fourth_layer.parameters())\n",
    "\n",
    "hidden_units1 = 10\n",
    "output_units = 26\n",
    "\n",
    "# Lookup matrix, reduce the dimension of 320\n",
    "lookup = torch.randn((320, hidden_units1)) * 0.01\n",
    "bias_lookup = torch.randn(hidden_units1) * 0.01\n",
    "\n",
    "# The hidden layer\n",
    "weight = torch.randn((hidden_units1, output_units))  * 0.01\n",
    "bias = torch.randn(output_units) * 0.01\n",
    "\n",
    "parameters += [lookup, bias_lookup, weight, bias]\n",
    "\n",
    "for parameter in parameters:\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_func(minibatch):\n",
    "    # Minibacth is of shape N * 28 * 28\n",
    "    batch_size = minibatch.shape[0]\n",
    "\n",
    "    # Measure the execution time\n",
    "    start_time = time.time()\n",
    "\n",
    "    minibatch = minibatch.view(batch_size, 1, 28, 28)\n",
    "    minibatch = F.unfold(minibatch, kernel_size=4, padding=2, stride=4).transpose(1, 2).view(batch_size, 8, 8, 16)\n",
    "    minibatch = torch.stack([first_layer(batch) for batch in minibatch])\n",
    "\n",
    "    print(\"use time {0} in the first layer\".format(time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Minibacth is of shape N * 8 * 16\n",
    "    minibatch = minibatch.view(batch_size, 1, 8, 16)\n",
    "    minibatch = F.unfold(minibatch, kernel_size=2, stride=2).transpose(1, 2).view(batch_size, 4, 8, 4)\n",
    "    minibatch = torch.stack([second_layer(batch) for batch in minibatch])\n",
    "\n",
    "    print(\"use time {0} in the second layer\".format(time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Minibatch is of shape N * 4 * 32\n",
    "    minibatch = minibatch.view(batch_size, 1, 4, 32)\n",
    "    minibatch = F.unfold(minibatch, kernel_size=2, stride=2).transpose(1, 2).view(batch_size, 2, 16, 4)\n",
    "    minibatch = torch.stack([third_layer(batch) for batch in minibatch])\n",
    "\n",
    "    print(\"use time {0} in the third layer\".format(time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Minibatch is of shape N * 2 * 128\n",
    "    minibatch = minibatch.view(batch_size, 1, 2, 128)\n",
    "    minibatch = F.unfold(minibatch, kernel_size=2, stride=2).transpose(1, 2).view(batch_size, 1, 64, 4)\n",
    "    minibatch = torch.stack([fourth_layer(batch) for batch in minibatch])\n",
    "\n",
    "    print(\"use time {0} in the fourth layer\".format(time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "\n",
    "    minibatch = minibatch.view(batch_size, 320)\n",
    "\n",
    "    return (minibatch @ lookup + bias_lookup).tanh() @ weight + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use time 2.1435163021087646 in the first layer\n",
      "use time 0.8712856769561768 in the second layer\n",
      "use time 0.7503175735473633 in the third layer\n",
      "use time 0.979649543762207 in the fourth layer\n",
      "use time 1.797321081161499 in the first layer\n",
      "use time 0.7510554790496826 in the second layer\n",
      "use time 0.6955010890960693 in the third layer\n",
      "use time 1.0789389610290527 in the fourth layer\n",
      "use time 2.4813265800476074 in the first layer\n",
      "use time 0.9182412624359131 in the second layer\n",
      "use time 0.7667310237884521 in the third layer\n",
      "use time 1.0135552883148193 in the fourth layer\n",
      "use time 1.9987881183624268 in the first layer\n",
      "use time 0.7673602104187012 in the second layer\n",
      "use time 0.5740048885345459 in the third layer\n",
      "use time 1.0702898502349854 in the fourth layer\n",
      "use time 2.664102554321289 in the first layer\n",
      "use time 0.9586710929870605 in the second layer\n",
      "use time 0.691685676574707 in the third layer\n",
      "use time 1.144834041595459 in the fourth layer\n",
      "use time 1.9518535137176514 in the first layer\n",
      "use time 0.7695941925048828 in the second layer\n",
      "use time 0.5971319675445557 in the third layer\n",
      "use time 1.0768496990203857 in the fourth layer\n",
      "use time 2.8707945346832275 in the first layer\n",
      "use time 1.0072407722473145 in the second layer\n",
      "use time 0.7150881290435791 in the third layer\n",
      "use time 1.1682007312774658 in the fourth layer\n",
      "use time 2.079754114151001 in the first layer\n",
      "use time 0.6841645240783691 in the second layer\n",
      "use time 0.7086849212646484 in the third layer\n",
      "use time 1.053532600402832 in the fourth layer\n",
      "use time 2.7523763179779053 in the first layer\n",
      "use time 0.8690862655639648 in the second layer\n",
      "use time 0.8222055435180664 in the third layer\n",
      "use time 1.0625593662261963 in the fourth layer\n",
      "use time 2.168858289718628 in the first layer\n",
      "use time 0.7871863842010498 in the second layer\n",
      "use time 0.5916788578033447 in the third layer\n",
      "use time 1.0492103099822998 in the fourth layer\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "learning_rates = 10**torch.linspace(-3.0, 0.0, 100)\n",
    "\n",
    "rate_i, loss_i = [], []\n",
    "\n",
    "# Randomly select batch from the dataset\n",
    "for _ in range(10):\n",
    "    selected = torch.randint(0, x_train.shape[0], size=(batch_size,))\n",
    "    batch_x = forward_func(x_train[selected])\n",
    "    batch_y = y_train[selected]\n",
    "    loss = F.cross_entropy(batch_x, batch_y)\n",
    "    loss.backward()\n",
    "\n",
    "    # learning_rate = learning_rates[_]\n",
    "    # rate_i.append(learning_rate)\n",
    "    # loss_i.append(loss.item())\n",
    "    for parameter in parameters:\n",
    "        parameter.data -= parameter.grad\n",
    "        parameter.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2639, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "- The whole process takes too much time, making it impossible to perform back propogation effeciently.\n",
    "- The loss function does not seem to reduce.\n",
    "- Some parameters are having too small impact on the loss fuction, $~e^{-17}$ waste\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
